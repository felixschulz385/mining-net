{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da021d74",
   "metadata": {},
   "source": [
    "# Convert Zarr Data to PyTorch MemoryMappedTensor Format\n",
    "\n",
    "This notebook converts existing Zarr-stored tiles to PyTorch's MemoryMappedTensor format for faster data loading.\n",
    "\n",
    "**Benefits of MemoryMappedTensor:**\n",
    "- Zero-copy memory mapping for instant access\n",
    "- Native PyTorch integration (no conversion overhead)\n",
    "- Extremely fast random access\n",
    "- Lower memory footprint during training\n",
    "- Direct GPU transfer without intermediate copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cebd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from data.database import DownloadDatabase\n",
    "from data.config import Config\n",
    "from data.tasks import compute_cluster_id\n",
    "from odc.geo.geobox import GeoBox, GeoboxTiles\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aefc50",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b6f05",
   "metadata": {},
   "source": [
    "## Database Migration (Old to New Schema)\n",
    "\n",
    "If you're upgrading from the old database schema, run this cell first to add the new columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1349af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migrating database: C:\\Users\\schulz0022\\Documents\\mining-net\\data\\mining_segmentation.db\n",
      "\n",
      "Current columns in tiles table: {'year', 'written_at', 'tile_ix', 'cluster_id', 'zarr_written', 'created_at', 'geometry_hash', 'tile_iy'}\n",
      "\n",
      "Adding columns: ['mmap_path TEXT', 'mmap_written BOOLEAN DEFAULT 0', 'mmap_written_at TEXT']\n",
      "  ✓ Added: mmap_path TEXT\n",
      "  ✓ Added: mmap_written BOOLEAN DEFAULT 0\n",
      "  ✓ Added: mmap_written_at TEXT\n",
      "  ✓ Created index: idx_tiles_mmap_written\n",
      "  ✓ Created index: idx_tiles_mmap_path\n",
      "\n",
      "✓ Migration completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "def migrate_database_schema(db_path: str):\n",
    "    \"\"\"\n",
    "    Migrate database from old schema to new schema.\n",
    "    Adds mmap_path and mmap_written columns to tiles table if they don't exist.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the SQLite database file\n",
    "    \"\"\"\n",
    "    db_path = Path(db_path)\n",
    "    if not db_path.exists():\n",
    "        print(f\"Database does not exist: {db_path}\")\n",
    "        return\n",
    "    \n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Get current table info\n",
    "        cursor.execute(\"PRAGMA table_info(tiles)\")\n",
    "        columns = {row[1] for row in cursor.fetchall()}\n",
    "        \n",
    "        print(f\"Current columns in tiles table: {columns}\")\n",
    "        \n",
    "        # Add missing columns if they don't exist\n",
    "        columns_to_add = []\n",
    "        \n",
    "        if 'mmap_path' not in columns:\n",
    "            columns_to_add.append(\"mmap_path TEXT\")\n",
    "        \n",
    "        if 'mmap_written' not in columns:\n",
    "            columns_to_add.append(\"mmap_written BOOLEAN DEFAULT 0\")\n",
    "        \n",
    "        if 'mmap_written_at' not in columns:\n",
    "            columns_to_add.append(\"mmap_written_at TEXT\")\n",
    "        \n",
    "        if columns_to_add:\n",
    "            print(f\"\\nAdding columns: {columns_to_add}\")\n",
    "            \n",
    "            for column_def in columns_to_add:\n",
    "                alter_sql = f\"ALTER TABLE tiles ADD COLUMN {column_def}\"\n",
    "                try:\n",
    "                    cursor.execute(alter_sql)\n",
    "                    print(f\"  ✓ Added: {column_def}\")\n",
    "                except sqlite3.OperationalError as e:\n",
    "                    print(f\"  ✗ Failed to add {column_def}: {e}\")\n",
    "            \n",
    "            # Create indices on new columns\n",
    "            index_commands = [\n",
    "                (\"idx_tiles_mmap_written\", \"ON tiles(mmap_written)\"),\n",
    "                (\"idx_tiles_mmap_path\", \"ON tiles(mmap_path)\")\n",
    "            ]\n",
    "            \n",
    "            for idx_name, idx_def in index_commands:\n",
    "                try:\n",
    "                    cursor.execute(f\"CREATE INDEX IF NOT EXISTS {idx_name} {idx_def}\")\n",
    "                    print(f\"  ✓ Created index: {idx_name}\")\n",
    "                except sqlite3.OperationalError as e:\n",
    "                    print(f\"  ✗ Failed to create index {idx_name}: {e}\")\n",
    "            \n",
    "            conn.commit()\n",
    "            print(\"\\n✓ Migration completed successfully!\")\n",
    "        else:\n",
    "            print(\"✓ Schema is already up to date. No migration needed.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Migration failed: {e}\")\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "# Perform migration\n",
    "if config.DB_PATH.exists():\n",
    "    print(f\"Migrating database: {config.DB_PATH}\\n\")\n",
    "    migrate_database_schema(str(config.DB_PATH))\n",
    "else:\n",
    "    print(f\"Database will be created fresh: {config.DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b39cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zarr source: C:\\Users\\schulz0022\\Documents\\mining-net\\data\\global_landsat.zarr\n",
      "Memory-mapped output: C:\\Users\\schulz0022\\Documents\\mining-net\\data\\landsat_mmap\n",
      "Bands: ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'thermal']\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "config = Config()\n",
    "db = DownloadDatabase(str(config.DB_PATH))\n",
    "\n",
    "# Paths\n",
    "ZARR_PATH = config.DATA_DIR / \"global_landsat.zarr\"\n",
    "MMAP_DIR = config.DATA_DIR / \"landsat_mmap\"\n",
    "MMAP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Bands to convert\n",
    "BANDS = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'thermal']\n",
    "\n",
    "print(f\"Zarr source: {ZARR_PATH}\")\n",
    "print(f\"Memory-mapped output: {MMAP_DIR}\")\n",
    "print(f\"Bands: {BANDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea364c80",
   "metadata": {},
   "source": [
    "## Load Zarr Dataset and Get Tiles to Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ad159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zarr dataset loaded:\n",
      "<xarray.Dataset> Size: 26TB\n",
      "Dimensions:           (latitude: 667916, longitude: 1335832)\n",
      "Coordinates:\n",
      "  * latitude          (latitude) float64 5MB 90.0 90.0 90.0 ... -90.0 -90.0\n",
      "  * longitude         (longitude) float64 11MB -180.0 -180.0 ... 180.0 180.0\n",
      "Data variables:\n",
      "    swir1             (latitude, longitude) float32 4TB dask.array<chunksize=(5632, 5632), meta=np.ndarray>\n",
      "    blue              (latitude, longitude) float32 4TB dask.array<chunksize=(5632, 5632), meta=np.ndarray>\n",
      "    green             (latitude, longitude) float32 4TB dask.array<chunksize=(5632, 5632), meta=np.ndarray>\n",
      "    nir               (latitude, longitude) float32 4TB dask.array<chunksize=(5632, 5632), meta=np.ndarray>\n",
      "    red               (latitude, longitude) float32 4TB dask.array<chunksize=(5632, 5632), meta=np.ndarray>\n",
      "    mining_footprint  (latitude, longitude) uint8 892GB dask.array<chunksize=(11264, 11264), meta=np.ndarray>\n",
      "    swir2             (latitude, longitude) float32 4TB dask.array<chunksize=(5632, 5632), meta=np.ndarray>\n",
      "    thermal           (latitude, longitude) float32 4TB dask.array<chunksize=(5632, 5632), meta=np.ndarray>\n",
      "Attributes:\n",
      "    crs:      EPSG:4326\n"
     ]
    }
   ],
   "source": [
    "# Open zarr dataset\n",
    "zarr_ds = xr.open_zarr(str(ZARR_PATH), consolidated=False, chunks='auto')\n",
    "\n",
    "print(\"Zarr dataset loaded:\")\n",
    "print(zarr_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d80b6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34931 tiles to convert\n",
      "\n",
      "Sample tile: {'tile_ix': 7043, 'tile_iy': 12013, 'cluster_id': 0, 'year': 2019, 'country_code': 'ZAF', 'geometry_hash': '23fcc4766d9bf67565d3221bd0313d14149927ac5e6182b34b8a6c859a104d68'}\n"
     ]
    }
   ],
   "source": [
    "# Get all written tiles from database with country code for cluster ID computation\n",
    "with db.get_connection() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT t.tile_ix, t.tile_iy, t.cluster_id, t.year, \n",
    "               tasks.country_code, tasks.geometry_hash, tasks.mining_footprint_json\n",
    "        FROM tiles t\n",
    "        JOIN tasks ON t.geometry_hash = tasks.geometry_hash \n",
    "            AND t.year = tasks.year\n",
    "        WHERE t.zarr_written = 1\n",
    "        ORDER BY t.cluster_id, t.year, t.tile_ix, t.tile_iy\n",
    "    \"\"\")\n",
    "    tiles = [dict(row) for row in cursor.fetchall()]\n",
    "\n",
    "print(f\"Found {len(tiles)} tiles to convert\")\n",
    "if tiles:\n",
    "    sample = tiles[0]\n",
    "    print(f\"\\nSample tile: {sample}\")\n",
    "    print(f\"  Old cluster_id: {sample['cluster_id']}\")\n",
    "    print(f\"  Country code: {sample['country_code']}\")\n",
    "    print(f\"  Has mining_footprint: {sample['mining_footprint_json'] is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a133768d",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d2fdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_tile_from_zarr(\n",
    "    tile_ix: int,\n",
    "    tile_iy: int,\n",
    "    bands: list,\n",
    "    include_footprint: bool = True\n",
    "):\n",
    "    \"\"\"Load tile data from Zarr.\"\"\"\n",
    "    # Reconstruct world geobox\n",
    "    world_geobox = GeoBox.from_bbox(\n",
    "        [-180, -90, 180, 90],\n",
    "        resolution=config.WORLD_GEOBOX_RESOLUTION,\n",
    "        crs=4326\n",
    "    )\n",
    "    world_geobox_tiles = GeoboxTiles(\n",
    "        world_geobox,\n",
    "        tile_shape=config.WORLD_GEOBOX_TILE_SIZE\n",
    "    )\n",
    "    \n",
    "    # Get tile geobox\n",
    "    tile_geobox = world_geobox_tiles[tile_ix, tile_iy]\n",
    "    bounds = tile_geobox.boundingbox\n",
    "    \n",
    "    # Load band data\n",
    "    band_arrays = [\n",
    "        zarr_ds[band].sel(\n",
    "            latitude=slice(bounds.top, bounds.bottom),\n",
    "            longitude=slice(bounds.left, bounds.right)\n",
    "        ).values\n",
    "        for band in bands\n",
    "    ]\n",
    "    \n",
    "    # Stack to (H, W, C)\n",
    "    features = np.stack(band_arrays, axis=-1).astype(np.float32)\n",
    "    \n",
    "    # Load mining footprint\n",
    "    labels = None\n",
    "    if include_footprint:\n",
    "        labels = zarr_ds['mining_footprint'].sel(\n",
    "            latitude=slice(bounds.top, bounds.bottom),\n",
    "            longitude=slice(bounds.left, bounds.right)\n",
    "        ).values\n",
    "        \n",
    "        if labels.ndim == 2:\n",
    "            labels = labels[..., np.newaxis]\n",
    "        \n",
    "        labels = labels.astype(np.float32)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def save_tile_as_mmap(\n",
    "    features: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    tile_ix: int,\n",
    "    tile_iy: int,\n",
    "    year: int,\n",
    "    cluster_id: int,\n",
    "    geometry_hash: str,\n",
    "    country_code: str,\n",
    "    mining_footprint_json: dict,\n",
    "    output_dir: Path\n",
    ") -> Path:\n",
    "    \"\"\"Save tile as memory-mapped PyTorch tensors.\n",
    "    \n",
    "    Args:\n",
    "        features: Feature array (H, W, C)\n",
    "        labels: Label array (H, W, C)\n",
    "        tile_ix: Tile X index\n",
    "        tile_iy: Tile Y index\n",
    "        year: Year\n",
    "        cluster_id: Original (old) cluster ID\n",
    "        geometry_hash: Geometry hash\n",
    "        country_code: ISO3 country code\n",
    "        mining_footprint_json: Mining footprint GeoJSON\n",
    "        output_dir: Output directory root\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (tile_dir, new_cluster_id)\n",
    "    \"\"\"\n",
    "    # Compute new globally unique cluster ID from country + mining footprint\n",
    "    new_cluster_id = compute_cluster_id(country_code, cluster_id, mining_footprint_json)\n",
    "    \n",
    "    # Create tile directory: new_cluster_id/year/tile_ix_tile_iy/\n",
    "    tile_dir = output_dir / str(new_cluster_id) / str(year) / f\"{tile_ix}_{tile_iy}\"\n",
    "    tile_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Replace NaN with 0\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    labels = np.nan_to_num(labels, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Convert to torch tensors: (H, W, C) -> (C, H, W)\n",
    "    features_tensor = torch.from_numpy(features).float().permute(2, 0, 1)\n",
    "    labels_tensor = torch.from_numpy(labels).float().permute(2, 0, 1)\n",
    "    \n",
    "    # Save as memory-mapped tensors\n",
    "    features_path = tile_dir / \"features.pt\"\n",
    "    labels_path = tile_dir / \"labels.pt\"\n",
    "    \n",
    "    # Save with shared memory for fast loading\n",
    "    torch.save(features_tensor, features_path)\n",
    "    torch.save(labels_tensor, labels_path)\n",
    "    \n",
    "    # Save metadata (without mmap_written_at - database tracks that)\n",
    "    metadata = {\n",
    "        \"tile_ix\": tile_ix,\n",
    "        \"tile_iy\": tile_iy,\n",
    "        \"year\": year,\n",
    "        \"cluster_id_old\": cluster_id,\n",
    "        \"cluster_id_new\": new_cluster_id,\n",
    "        \"geometry_hash\": geometry_hash,\n",
    "        \"country_code\": country_code,\n",
    "        \"features_shape\": list(features_tensor.shape),\n",
    "        \"labels_shape\": list(labels_tensor.shape),\n",
    "        \"bands\": BANDS,\n",
    "        \"dtype\": \"float32\"\n",
    "    }\n",
    "    \n",
    "    metadata_path = tile_dir / \"metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    return tile_dir, new_cluster_id\n",
    "\n",
    "\n",
    "def update_database_mmap_status(\n",
    "    tile_ix: int,\n",
    "    tile_iy: int,\n",
    "    geometry_hash: str,\n",
    "    year: int,\n",
    "    new_cluster_id: int\n",
    "):\n",
    "    \"\"\"Update database to track memory-mapped tile with new cluster ID.\n",
    "    \n",
    "    Args:\n",
    "        tile_ix: Tile X index\n",
    "        tile_iy: Tile Y index\n",
    "        geometry_hash: Geometry hash\n",
    "        year: Year\n",
    "        new_cluster_id: New globally unique cluster ID\n",
    "    \"\"\"\n",
    "    with db.get_connection() as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            UPDATE tiles \n",
    "            SET cluster_id = ?, mmap_written = 1, mmap_written_at = datetime('now')\n",
    "            WHERE tile_ix = ? AND tile_iy = ? \n",
    "              AND geometry_hash = ? AND year = ?\n",
    "        \"\"\", (new_cluster_id, tile_ix, tile_iy, geometry_hash, year))\n",
    "\n",
    "print(\"Helper functions defined with cluster ID migration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106c087",
   "metadata": {},
   "source": [
    "## Convert Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e12915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with tile: {'tile_ix': 7043, 'tile_iy': 12013, 'cluster_id': 0, 'year': 2019, 'country_code': 'ZAF', 'geometry_hash': '23fcc4766d9bf67565d3221bd0313d14149927ac5e6182b34b8a6c859a104d68'}\n",
      "Loaded features shape: (64, 64, 7), dtype: float32\n",
      "Loaded labels shape: (64, 64, 1), dtype: float32\n",
      "\n",
      "Saved to: C:\\Users\\schulz0022\\Documents\\mining-net\\data\\landsat_mmap\\0\\2019\\7043_12013\n",
      "Files: [WindowsPath('C:/Users/schulz0022/Documents/mining-net/data/landsat_mmap/0/2019/7043_12013/features.pt'), WindowsPath('C:/Users/schulz0022/Documents/mining-net/data/landsat_mmap/0/2019/7043_12013/labels.pt'), WindowsPath('C:/Users/schulz0022/Documents/mining-net/data/landsat_mmap/0/2019/7043_12013/metadata.json')]\n",
      "\n",
      "Loaded back features shape: torch.Size([7, 64, 64])\n",
      "Loaded back labels shape: torch.Size([1, 64, 64])\n",
      "\n",
      "Verification: shapes match = True\n",
      "\n",
      "10 loads took 0.004s (0.36ms per load)\n"
     ]
    }
   ],
   "source": [
    "# Test conversion on a single tile first\n",
    "if tiles:\n",
    "    test_tile = tiles[0]\n",
    "    print(f\"Testing with tile: {test_tile}\")\n",
    "    \n",
    "    # Load from zarr\n",
    "    features, labels = load_tile_from_zarr(\n",
    "        test_tile['tile_ix'],\n",
    "        test_tile['tile_iy'],\n",
    "        BANDS\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded features shape: {features.shape}, dtype: {features.dtype}\")\n",
    "    print(f\"Loaded labels shape: {labels.shape}, dtype: {labels.dtype}\")\n",
    "    \n",
    "    # Parse mining footprint\n",
    "    mining_footprint = json.loads(test_tile['mining_footprint_json']) if test_tile['mining_footprint_json'] else None\n",
    "    \n",
    "    # Save as mmap (computes new cluster ID)\n",
    "    tile_dir, new_cluster_id = save_tile_as_mmap(\n",
    "        features, labels,\n",
    "        test_tile['tile_ix'], test_tile['tile_iy'],\n",
    "        test_tile['year'], test_tile['cluster_id'],\n",
    "        test_tile['geometry_hash'],\n",
    "        test_tile['country_code'],\n",
    "        mining_footprint,\n",
    "        MMAP_DIR\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCluster ID migration:\")\n",
    "    print(f\"  Old: {test_tile['cluster_id']}\")\n",
    "    print(f\"  New: {new_cluster_id}\")\n",
    "    print(f\"  Saved to: {tile_dir}\")\n",
    "    print(f\"  Files: {list(tile_dir.iterdir())}\")\n",
    "    \n",
    "    # Verify by loading back\n",
    "    loaded_features = torch.load(tile_dir / \"features.pt\")\n",
    "    loaded_labels = torch.load(tile_dir / \"labels.pt\")\n",
    "    \n",
    "    print(f\"\\nLoaded back features shape: {loaded_features.shape}\")\n",
    "    print(f\"Loaded back labels shape: {loaded_labels.shape}\")\n",
    "    print(f\"\\nVerification: shapes match = {loaded_features.shape == torch.Size([len(BANDS), features.shape[0], features.shape[1]])}\")\n",
    "    \n",
    "    # Check metadata doesn't include mmap_written_at\n",
    "    with open(tile_dir / \"metadata.json\") as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"\\nMetadata keys: {list(metadata.keys())}\")\n",
    "    print(f\"Has mmap_written_at: {'mmap_written_at' in metadata} (should be False - tracked in DB)\")\n",
    "    \n",
    "    # Show memory-mapped access speed\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = torch.load(tile_dir / \"features.pt\")\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"\\n10 loads took {elapsed:.3f}s ({elapsed/10*1000:.2f}ms per load)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213e540",
   "metadata": {},
   "source": [
    "## Batch Convert All Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f03a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 34931 tiles...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df955c75bd1d4980bcfad48402ffb8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting tiles:   0%|          | 0/34931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Conversion complete!\n",
      "Successfully converted: 34931\n",
      "Errors: 0\n"
     ]
    }
   ],
   "source": [
    "# Convert all tiles\n",
    "converted_count = 0\n",
    "error_count = 0\n",
    "errors = []\n",
    "cluster_id_migrations = {}  # Track old -> new cluster ID mappings\n",
    "\n",
    "print(f\"Converting {len(tiles)} tiles...\\n\")\n",
    "\n",
    "for tile in tqdm(tiles, desc=\"Converting tiles\"):\n",
    "    try:\n",
    "        # Load from zarr\n",
    "        features, labels = load_tile_from_zarr(\n",
    "            tile['tile_ix'],\n",
    "            tile['tile_iy'],\n",
    "            BANDS\n",
    "        )\n",
    "        \n",
    "        # Parse mining footprint\n",
    "        mining_footprint = json.loads(tile['mining_footprint_json']) if tile['mining_footprint_json'] else None\n",
    "        \n",
    "        # Save as mmap (computes new cluster ID)\n",
    "        tile_dir, new_cluster_id = save_tile_as_mmap(\n",
    "            features, labels,\n",
    "            tile['tile_ix'], tile['tile_iy'],\n",
    "            tile['year'], tile['cluster_id'],\n",
    "            tile['geometry_hash'],\n",
    "            tile['country_code'],\n",
    "            mining_footprint,\n",
    "            MMAP_DIR\n",
    "        )\n",
    "        \n",
    "        # Track cluster ID migration\n",
    "        old_id = tile['cluster_id']\n",
    "        if old_id not in cluster_id_migrations:\n",
    "            cluster_id_migrations[old_id] = new_cluster_id\n",
    "        \n",
    "        # Update database with new cluster ID\n",
    "        update_database_mmap_status(\n",
    "            tile['tile_ix'], tile['tile_iy'],\n",
    "            tile['geometry_hash'], tile['year'],\n",
    "            new_cluster_id\n",
    "        )\n",
    "        \n",
    "        converted_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        errors.append({\n",
    "            'tile': tile,\n",
    "            'error': str(e)\n",
    "        })\n",
    "        logger.error(f\"Error converting tile {tile}: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Conversion complete!\")\n",
    "print(f\"Successfully converted: {converted_count}\")\n",
    "print(f\"Errors: {error_count}\")\n",
    "print(f\"Unique old cluster IDs migrated: {len(cluster_id_migrations)}\")\n",
    "\n",
    "if cluster_id_migrations:\n",
    "    print(f\"\\nCluster ID migrations (first 5):\")\n",
    "    for old_id, new_id in list(cluster_id_migrations.items())[:5]:\n",
    "        print(f\"  {old_id} → {new_id}\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\nFirst few errors:\")\n",
    "    for err in errors[:5]:\n",
    "        print(f\"  Tile {err['tile']}: {err['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae56e73e",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33bd9ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Comparison (20 iterations):\n",
      "  Zarr + conversion:  58.91 ms/tile\n",
      "  MemoryMapped:       1.42 ms/tile\n",
      "  Speedup:            41.49x faster\n"
     ]
    }
   ],
   "source": [
    "# Compare load times: Zarr vs MemoryMapped\n",
    "import time\n",
    "\n",
    "if tiles:\n",
    "    test_tile = tiles[0]\n",
    "    n_iterations = 20\n",
    "    \n",
    "    # Time Zarr loading\n",
    "    start = time.time()\n",
    "    for _ in range(n_iterations):\n",
    "        features, labels = load_tile_from_zarr(\n",
    "            test_tile['tile_ix'],\n",
    "            test_tile['tile_iy'],\n",
    "            BANDS\n",
    "        )\n",
    "        # Convert to torch (what data loader does)\n",
    "        features_t = torch.from_numpy(features).float().permute(2, 0, 1)\n",
    "        labels_t = torch.from_numpy(labels).float().permute(2, 0, 1)\n",
    "    zarr_time = (time.time() - start) / n_iterations\n",
    "    \n",
    "    # Time MemoryMapped loading\n",
    "    tile_dir = MMAP_DIR / str(test_tile['cluster_id']) / str(test_tile['year']) / f\"{test_tile['tile_ix']}_{test_tile['tile_iy']}\"\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(n_iterations):\n",
    "        features_t = torch.load(tile_dir / \"features.pt\")\n",
    "        labels_t = torch.load(tile_dir / \"labels.pt\")\n",
    "    mmap_time = (time.time() - start) / n_iterations\n",
    "    \n",
    "    speedup = zarr_time / mmap_time\n",
    "    \n",
    "    print(f\"Performance Comparison ({n_iterations} iterations):\")\n",
    "    print(f\"  Zarr + conversion:  {zarr_time*1000:.2f} ms/tile\")\n",
    "    print(f\"  MemoryMapped:       {mmap_time*1000:.2f} ms/tile\")\n",
    "    print(f\"  Speedup:            {speedup:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165776b8",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c5b099",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Verify the conversion was successful and check data integrity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8054e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying MMAP conversion...\n",
      "\n",
      "Database Statistics:\n",
      "  Tiles marked as mmap_written: 0\n",
      "  Tiles with mmap_path: 0\n",
      "\n",
      "File System Statistics:\n",
      "  MMAP directory: C:\\Users\\schulz0022\\Documents\\mining-net\\data\\landsat_mmap\n",
      "  MMAP directory size: 4.38 GB\n",
      "  ✓ All tiles have metadata.json\n",
      "  ✓ Metadata sanity check: 10 tiles checked\n",
      "    - Has mmap_written_at in metadata: 0 (should be 0 - tracked in DB)\n",
      "\n",
      "============================================================\n",
      "Conversion Status: ✗ INCOMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify MMAP conversion\n",
    "print(\"Verifying MMAP conversion...\\n\")\n",
    "\n",
    "with db.get_connection() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Count mmap_written tiles\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM tiles WHERE mmap_written = 1\")\n",
    "    mmap_count = cursor.fetchone()[0]\n",
    "    \n",
    "    # Get sample mmap tiles\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT tile_ix, tile_iy, year, cluster_id\n",
    "        FROM tiles\n",
    "        WHERE mmap_written = 1\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    samples = cursor.fetchall()\n",
    "\n",
    "print(f\"Database Statistics:\")\n",
    "print(f\"  Tiles marked as mmap_written: {mmap_count}\")\n",
    "\n",
    "if samples:\n",
    "    print(f\"\\nSample tiles (with new cluster IDs):\")\n",
    "    for sample in samples:\n",
    "        print(f\"  Tile {sample[0]}_{sample[1]} (year={sample[2]}, cluster_id={sample[3]})\")\n",
    "        # Path can be reconstructed: {cluster_id}/{year}/{tile_ix}_{tile_iy}/\n",
    "        path = MMAP_DIR / str(sample[3]) / str(sample[2]) / f\"{sample[0]}_{sample[1]}\"\n",
    "        print(f\"    Path: {path}\")\n",
    "\n",
    "# Verify file integrity\n",
    "print(f\"\\nFile System Statistics:\")\n",
    "print(f\"  MMAP directory: {MMAP_DIR}\")\n",
    "print(f\"  MMAP directory size: {get_dir_size(MMAP_DIR) / (1024**3):.2f} GB\")\n",
    "\n",
    "# Check for missing metadata files\n",
    "missing_metadata = 0\n",
    "for cluster_dir in MMAP_DIR.iterdir():\n",
    "    if not cluster_dir.is_dir():\n",
    "        continue\n",
    "    for year_dir in cluster_dir.iterdir():\n",
    "        if not year_dir.is_dir():\n",
    "            continue\n",
    "        for tile_dir in year_dir.iterdir():\n",
    "            if not tile_dir.is_dir():\n",
    "                continue\n",
    "            if not (tile_dir / \"metadata.json\").exists():\n",
    "                missing_metadata += 1\n",
    "\n",
    "if missing_metadata == 0:\n",
    "    print(f\"  ✓ All tiles have metadata.json\")\n",
    "else:\n",
    "    print(f\"  ✗ Missing metadata: {missing_metadata} tiles\")\n",
    "\n",
    "# Verify metadata doesn't store mmap_written_at (should be in DB only)\n",
    "checked_metadata_count = 0\n",
    "has_mmap_written_at = 0\n",
    "\n",
    "for cluster_dir in MMAP_DIR.iterdir():\n",
    "    if not cluster_dir.is_dir():\n",
    "        continue\n",
    "    for year_dir in cluster_dir.iterdir():\n",
    "        if not year_dir.is_dir():\n",
    "            continue\n",
    "        for tile_dir in year_dir.iterdir():\n",
    "            if not tile_dir.is_dir():\n",
    "                continue\n",
    "            metadata_path = tile_dir / \"metadata.json\"\n",
    "            if metadata_path.exists():\n",
    "                checked_metadata_count += 1\n",
    "                with open(metadata_path) as f:\n",
    "                    metadata = json.load(f)\n",
    "                if 'mmap_written_at' in metadata:\n",
    "                    has_mmap_written_at += 1\n",
    "            if checked_metadata_count >= 10:\n",
    "                break\n",
    "        if checked_metadata_count >= 10:\n",
    "            break\n",
    "    if checked_metadata_count >= 10:\n",
    "        break\n",
    "\n",
    "print(f\"  ✓ Metadata sanity check: {checked_metadata_count} tiles checked\")\n",
    "print(f\"    - Has mmap_written_at in metadata: {has_mmap_written_at} (should be 0 - tracked in DB)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Conversion Status: {'✓ COMPLETE' if mmap_count > 0 else '✗ INCOMPLETE'}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7806c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONVERSION SUMMARY\n",
      "============================================================\n",
      "Source:              C:\\Users\\schulz0022\\Documents\\mining-net\\data\\global_landsat.zarr\n",
      "Destination:         C:\\Users\\schulz0022\\Documents\\mining-net\\data\\landsat_mmap\n",
      "Tiles converted:     34931\n",
      "Errors:              0\n",
      "Total size:          4.38 GB\n",
      "Avg per tile:        0.13 MB\n",
      "Bands:               ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'thermal']\n",
      "Format:              PyTorch MemoryMappedTensor\n",
      "Index file:          C:\\Users\\schulz0022\\Documents\\mining-net\\data\\landsat_mmap\\index.json\n",
      "\n",
      "Next steps:\n",
      "  1. Update database schema (see update_database_mmap_status)\n",
      "  2. Update data_loader.py to use new format\n",
      "  3. Test training with new data loader\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate storage sizes\n",
    "import os\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total += os.path.getsize(filepath)\n",
    "    return total\n",
    "\n",
    "mmap_size = get_dir_size(MMAP_DIR)\n",
    "mmap_size_gb = mmap_size / (1024**3)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CONVERSION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Source:              {ZARR_PATH}\")\n",
    "print(f\"Destination:         {MMAP_DIR}\")\n",
    "print(f\"Tiles converted:     {converted_count}\")\n",
    "print(f\"Errors:              {error_count}\")\n",
    "print(f\"Total size:          {mmap_size_gb:.2f} GB\")\n",
    "print(f\"Avg per tile:        {mmap_size_gb/converted_count*1024:.2f} MB\" if converted_count > 0 else \"N/A\")\n",
    "print(f\"Bands:               {BANDS}\")\n",
    "print(f\"Format:              PyTorch MemoryMappedTensor\")\n",
    "print(f\"Index file:          {MMAP_DIR / 'index.json'}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Update database schema (see update_database_mmap_status)\")\n",
    "print(f\"  2. Update data_loader.py to use new format\")\n",
    "print(f\"  3. Test training with new data loader\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d2789",
   "metadata": {},
   "source": [
    "## Sync Database Status from Disk\n",
    "\n",
    "Update the database to mark all tiles as mmap_written based on what exists on disk. This is useful if:\n",
    "- You're running this after a conversion\n",
    "- The database is out of sync with the filesystem\n",
    "- You need to rebuild the database status from existing MMAP files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0b66705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syncing database status from MMAP files on disk...\n",
      "\n",
      "\n",
      "============================================================\n",
      "DATABASE SYNC COMPLETE\n",
      "============================================================\n",
      "Tiles found on disk:        34931\n",
      "Tiles updated in database:  34931\n",
      "Tiles not in database:      0\n",
      "Errors:                     0\n",
      "Cluster ID migrations:      0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def sync_database_from_disk(\n",
    "    db_path: str,\n",
    "    mmap_dir: str,\n",
    "    verbose: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Sync database status from what exists on disk.\n",
    "    \n",
    "    Scans the MMAP directory structure and updates the database with:\n",
    "    - mmap_written = 1 for each tile found on disk\n",
    "    - cluster_id = new cluster ID (from metadata.json)\n",
    "    - mmap_written_at = current timestamp\n",
    "    \n",
    "    This is useful when:\n",
    "    - Database is out of sync with filesystem\n",
    "    - Rebuilding database from existing MMAP files\n",
    "    - Verifying conversion completeness\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        mmap_dir: Path to MMAP output directory\n",
    "        verbose: Print progress information\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with update statistics\n",
    "    \"\"\"\n",
    "    db_path = Path(db_path)\n",
    "    mmap_dir = Path(mmap_dir)\n",
    "    \n",
    "    if not db_path.exists():\n",
    "        print(f\"✗ Database not found: {db_path}\")\n",
    "        return None\n",
    "    \n",
    "    if not mmap_dir.exists():\n",
    "        print(f\"✗ MMAP directory not found: {mmap_dir}\")\n",
    "        return None\n",
    "    \n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    stats = {\n",
    "        'tiles_found': 0,\n",
    "        'tiles_updated': 0,\n",
    "        'tiles_not_in_db': 0,\n",
    "        'errors': 0,\n",
    "        'cluster_migrations': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Scan MMAP directory structure: cluster_id/year/tile_ix_tile_iy/\n",
    "        for cluster_dir in sorted(mmap_dir.iterdir()):\n",
    "            if not cluster_dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            cluster_id = int(cluster_dir.name)\n",
    "            \n",
    "            for year_dir in sorted(cluster_dir.iterdir()):\n",
    "                if not year_dir.is_dir():\n",
    "                    continue\n",
    "                \n",
    "                year = int(year_dir.name)\n",
    "                \n",
    "                for tile_dir in sorted(year_dir.iterdir()):\n",
    "                    if not tile_dir.is_dir():\n",
    "                        continue\n",
    "                    \n",
    "                    stats['tiles_found'] += 1\n",
    "                    \n",
    "                    # Parse tile coordinates from directory name\n",
    "                    tile_name = tile_dir.name\n",
    "                    try:\n",
    "                        tile_ix, tile_iy = map(int, tile_name.split('_'))\n",
    "                    except (ValueError, IndexError):\n",
    "                        if verbose:\n",
    "                            print(f\"  ✗ Could not parse tile name: {tile_name}\")\n",
    "                        stats['errors'] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Load metadata to get geometry_hash\n",
    "                    metadata_path = tile_dir / \"metadata.json\"\n",
    "                    if not metadata_path.exists():\n",
    "                        if verbose:\n",
    "                            print(f\"  ✗ No metadata for {tile_name}\")\n",
    "                        stats['errors'] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    with open(metadata_path, 'r') as f:\n",
    "                        metadata = json.load(f)\n",
    "                    \n",
    "                    geometry_hash = metadata.get('geometry_hash')\n",
    "                    if not geometry_hash:\n",
    "                        if verbose:\n",
    "                            print(f\"  ✗ No geometry_hash in metadata for {tile_name}\")\n",
    "                        stats['errors'] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Track cluster migrations\n",
    "                    old_cluster_id = metadata.get('cluster_id_old')\n",
    "                    new_cluster_id = metadata.get('cluster_id_new', cluster_id)\n",
    "                    if old_cluster_id and old_cluster_id not in stats['cluster_migrations']:\n",
    "                        stats['cluster_migrations'][old_cluster_id] = new_cluster_id\n",
    "                    \n",
    "                    # Update database\n",
    "                    try:\n",
    "                        cursor.execute(\"\"\"\n",
    "                            UPDATE tiles \n",
    "                            SET mmap_written = 1, \n",
    "                                cluster_id = ?,\n",
    "                                mmap_written_at = datetime('now')\n",
    "                            WHERE tile_ix = ? AND tile_iy = ? \n",
    "                              AND geometry_hash = ? AND year = ?\n",
    "                        \"\"\", (new_cluster_id, tile_ix, tile_iy, geometry_hash, year))\n",
    "                        \n",
    "                        if cursor.rowcount > 0:\n",
    "                            stats['tiles_updated'] += 1\n",
    "                        else:\n",
    "                            stats['tiles_not_in_db'] += 1\n",
    "                            if verbose:\n",
    "                                print(f\"  ⚠ Tile not in DB: {tile_ix}_{tile_iy} ({geometry_hash[:8]}, {year})\")\n",
    "                    except Exception as e:\n",
    "                        if verbose:\n",
    "                            print(f\"  ✗ Error updating {tile_name}: {e}\")\n",
    "                        stats['errors'] += 1\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DATABASE SYNC COMPLETE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Tiles found on disk:        {stats['tiles_found']}\")\n",
    "        print(f\"Tiles updated in database:  {stats['tiles_updated']}\")\n",
    "        print(f\"Tiles not in database:      {stats['tiles_not_in_db']}\")\n",
    "        print(f\"Errors:                     {stats['errors']}\")\n",
    "        print(f\"Cluster ID migrations:      {len(stats['cluster_migrations'])}\")\n",
    "        \n",
    "        if stats['cluster_migrations'] and verbose:\n",
    "            print(f\"\\nSample cluster migrations (old → new):\")\n",
    "            for old_id, new_id in list(stats['cluster_migrations'].items())[:3]:\n",
    "                print(f\"  {old_id} → {new_id}\")\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Sync failed: {e}\")\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# Run database sync from disk\n",
    "print(\"Syncing database status from MMAP files on disk...\\n\")\n",
    "sync_stats = sync_database_from_disk(\n",
    "    str(config.DB_PATH),\n",
    "    str(MMAP_DIR),\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
